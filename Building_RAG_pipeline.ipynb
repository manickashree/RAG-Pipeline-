{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a671b",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width:60px; float:right\"><br>\n",
    "# <font color=\"#76b900\">**Build a RAG Pipeline**<br/>with NVIDIA AI Foundation Models & Endpoints</font>\n",
    "\n",
    "**Welcome To Your Cloud Environment!** This interactive web application, which you're currently using to run Python code, is more than just a simple interface. When you access this Jupyter Notebook, an instance on a cloud platform is allocated to you by the [**NVIDIA Deep Learning Institute (DLI)**](https://www.nvidia.com/en-us/training/). This forms your base cloud environment, essentially a blank canvas for further setup, and includes:\n",
    "\n",
    "- A dedicated CPU, and possibly a GPU, for processing.\n",
    "- A pre-installed base operating system.\n",
    "- A pre-installation of packages necessary to run the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339574ad",
   "metadata": {},
   "source": [
    "### Learning Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db114955",
   "metadata": {},
   "source": [
    "In this tutorial, we will be building a **Chat-with-your-Documents RAG pipeline** using [**NVIDIA AI Foundation Models**](https://build.nvidia.com/explore/discover), accessed through endpoints hosted in NGC. These endpoints can be used by developers and data scientist to easily build PoC (proof of concept) applications that use hosted instances just like OpenAI.\n",
    "\n",
    "On the other hand, NVIDIA also provides **a suite of microservies (NIMs)**, where you can use your on-prem infra or DGX Cloud and move the exact same models into self-managed hosting by changing a few lines of code. NVIDIA microservices can scale out based on load, and run entirely on the GPUs: vector DB, embedding and LLM inference.\n",
    "\n",
    "Here, we will focus on building a RAG pipeline using NVIDIA AI Foundation Models and will learn:\n",
    "\n",
    "-  The components of a RAG pipeline\n",
    "    - Document loading\n",
    "    - Document preprocessing\n",
    "    - Generating embeddings for the chunked documents\n",
    "    - Indexing with Vector Database\n",
    "    - LLM (generator)\n",
    "- How to use *NVIDIA AI Foundation Endpoints* for easy access to hosted endpoints for generative AI models like Mistral, Llama-2, SteerLM, etc.\n",
    "- How to send a query and get a response from an LLM model\n",
    "- Why evaluation is a critical aspect of building and deploying RAG pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a7331",
   "metadata": {},
   "source": [
    "### Setting Your API Key\n",
    "\n",
    "In order to successfully run this notebook, you will need an **NVIDIA API KEY**.  If you did not create your API Key yet, please go through the setup steps in the [**Introduction**](./Introduction.ipynb) notebook and generate an API key with your own user account. You can supply the `NVIDIA_API_KEY` directly in this notebook when you run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb42a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to build.nvidia.com -> (Hosted Model of Choice) -> Get API Code or similar.\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d296f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**1. NVIDIA AI Foundation Endpoints**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e970ad",
   "metadata": {},
   "source": [
    "[**NVIDIA AI Foundation Endpoints**](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) give users easy access to NVIDIA hosted API endpoints for powerful models like Mixtral 8x7B, Llama 2, Stable Diffusion, etc. These models are optimized, tested, and hosted on the NVIDIA AI platform, making them fast and easy to evaluate, further customize, and seamlessly run at peak performance on any accelerated stack.\n",
    "\n",
    "- **For Designing:** With NVIDIA AI Foundation Endpoints, you can get quick results from a fully accelerated stack running on NVIDIA DGX Cloud. NVIDIA AI Foundation Models are freely available to experiment with now on the NVIDIA [**NGC catalog**](https://catalog.ngc.nvidia.com/ai-foundation-models) and Hugging Face.\n",
    "- **For Deploying:** Once you're ready to move to production, these models can be deployed anywhere with enterprise-grade security, stability, and support using [**NVIDIA AI Enterprise**](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/) or manually deployed on self-managed infrastructure using tools like [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) and [**NIM**](https://developer.nvidia.com/nemo-microservices-early-access).\n",
    "\n",
    "Using NeMo Retriever, enterprises can connect their LLMs to multiple data sources and knowledge bases, so that users can easily interact with data and receive accurate, up-to-date answers using simple, conversational prompts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35114dc9-77cd-46d8-bba8-575847c22c0c",
   "metadata": {},
   "source": [
    "### Getting Started With Langchain?\n",
    "\n",
    "We asked the question `What is Langchain?` to one of the strongest open-sourced LLM models out there: `mixtral_8x7b`. It generated the answer below:\n",
    "\n",
    "```\n",
    "In summary, LangChain is a decentralized platform that leverages blockchain technology to provide a secure and transparent marketplace for language services, while Langchain is a language learning platform that utilizes AI technology to provide personalized language learning content.\n",
    "```\n",
    "\n",
    "This does not look like the answer we are looking for, right? :) \n",
    "\n",
    "From its developers, [LangChain](https://github.com/langchain-ai/langchain) is defined as \"a framework for developing applications powered by language models.\" Available in Python and TypeScript (JS), the LangChain framework contains:\n",
    "- Interfaces and integrations for a variety of LLM building blocks (chain components).\n",
    "- A basic runtime scheme for combining these components into complex chains and agents.\n",
    "- Easy-to-use off-the-shelf implementations of useful LLM pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1aa5c",
   "metadata": {},
   "source": [
    "To help interface with this framework, the [**langchain-nvidia-ai-endpoints package**](https://github.com/langchain-ai/langchain-nvidia) provides connectors like [**`ChatNVIDIA`** ](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/langchain_nvidia_ai_endpoints/chat_models.py) and [**`NVIDIAEmbeddings`** ](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/langchain_nvidia_ai_endpoints/embeddings.py) to help interface with the raw endpoints. These will be used throughout the course to power our RAG pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27efe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Import the libraries\n",
    "import faiss\n",
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c733ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69dd6d-04bf-458c-8d38-e9f8ad5e1e0b",
   "metadata": {},
   "source": [
    "We initialize the llm model by calling the `ChatNVIDIA` class with our model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ebac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"ai-mistral-7b-instruct-v2\", nvidia_api_key=nvapi_key, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712076c9-6c6b-4c0c-a484-7afbf146da5b",
   "metadata": {},
   "source": [
    "From there, we can use LangChain chat model methods like `invoke` to generate the response or `stream` to pull response results as they generate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773dbda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Write a three line of poem about NVIDIA Nemo.\")\n",
    "print(result.content)\n",
    "\n",
    "# for token in llm.stream(\"Write a three line of poem about NVIDIA Nemo.\"):\n",
    "#     print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298c2c3",
   "metadata": {},
   "source": [
    "Looking at our list of models, a few general examples stand out: \n",
    "- `steerlm_llama_70b`: This model was trained with SteerLM approach developed by the NVIDIA NeMo Team, introduced as part of NVIDIA NeMo Alignment methods. It simplifies the customization of large language models (LLMs) and empowers users with dynamic control over model outputs by specifying desired attributes. You can read more about the model [here](https://arxiv.org/abs/2310.05344) and [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/modelalignment/steerlm.html). You can try `NV-Llama2-70B-SteerLM-Chat` on the [**NVIDIA NGC Catalog**](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/llama2-70b-steerlm).\n",
    "- `mixtral_8x7b-instruct`: When using this model, it'd be better to set the [instruct format](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#instruction-format) as recommended. Please keep in mind that multi-functional, more accurate large models are slower during inference & expensive to deploy. [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. `TensorRT-LLM` consists of the TensorRT deep learning compiler and includes optimized kernels, pre– and post-processing steps, and multi-GPU/multi-node communication primitives for groundbreaking performance on NVIDIA GPUs.\n",
    "\n",
    "For the purposes of this talk, we will mostly use Mixtral since it works nicely by defaults and is good about following instructions. This makes it a great candidate for as a language-reasoning backbone that can be guided by context! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195bd25",
   "metadata": {},
   "source": [
    "### Guiding LLM Generation\n",
    "\n",
    "In this talk, we will use our chosen LLM as a **\"generator\"** to give us some natural language responses. As these models have been trained to predict reasonable continuations and answer questions, they can be used as-is to predict average-human responses by default. \n",
    "\n",
    "Let's ask a question to our language model without providing the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389898a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = llm.invoke(\"who played Oppenheimer in the  Oppenheimer movie?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b38df",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is not the answer we are looking for, since this data is new and beyond its training data :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ef879",
   "metadata": {},
   "source": [
    "To help it out, let's provide our model some **context** that it can use and ask the question again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aea974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Oppenheimer is a 2023 epic biographical drama film written for the screen and directed by Christopher Nolan. \n",
    "It stars Cillian Murphy as Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb\" \n",
    "for his role in the Manhattan Project—the World War II undertaking that developed the first nuclear weapons. \n",
    "\n",
    "who played Oppenheimer in the  Oppenheimer movie?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e559a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32c2f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Yes, this time we got our answer! \n",
    "\n",
    "In this case, while the LLM is treated as a generator, the \"context\" is used to drive the generation in a desirable direction. This example just specifies it manually, but some other schemes could be used to pre-load this context or add surrounding instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5782d",
   "metadata": {},
   "source": [
    "### Constructing A Useful Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc95116",
   "metadata": {},
   "source": [
    "Now, let's do the same with a prompt template. \n",
    "- A **prompt template** is a pre-defined format that locks in certain aspects of a model component's inputs (i.e. instructions, question, etc.).\n",
    "- A **prompt** is the filled-in version of the template that is fed to the LLM to generate its output. \n",
    "\n",
    "We use [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#) class to create a prompt template for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e3e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Oppenheimer is a 2023 epic biographical drama film written for the screen and directed by Christopher Nolan. \n",
    "It stars Cillian Murphy as Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb\" \n",
    "for his role in the Manhattan Project—the World War II undertaking that developed the first nuclear weapons. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34175b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", \n",
    "        \"Answer solely based on the following context. Provide concise answer: {context}\"\n",
    "    ),\n",
    "    (\"user\", \"{question}\"),\n",
    "])\n",
    "\n",
    "model = ChatNVIDIA(model=\"ai-mistral-7b-instruct-v2\", max_tokens=1024)\n",
    "\n",
    "def print_return(d):\n",
    "    print(repr(d))\n",
    "    return d\n",
    "\n",
    "chain = (\n",
    "    {\"context\": lambda x: context, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | print_return  ## <- Include to see what gets passed to the model\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41de8b",
   "metadata": {},
   "source": [
    "When we call the chain on an input query, with the `print_return` function, we can also see the entire input goes into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf1710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(\"who played Oppenheimer in the movie Oppenheimer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2144e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5eb40f",
   "metadata": {},
   "source": [
    "**This time we got a concise answer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253fe85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(\"who is the producer in the Oppenheimer movie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774eeb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfd560",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Without the context, this is expected since the model does not have access to this information and was not trained on it.**\n",
    "\n",
    "Let's explain this further in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144efbc4",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**2. Retrieval-Augmented Generation (RAG) for Q&A**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7ecc4",
   "metadata": {},
   "source": [
    "What just happened? As you just experienced, LLMs might not be responding reasonably to every question we ask. Although Large Language Models (LLMs) show promising results in understanding and generating text, they might have certain issues such as hallucination, limitation of reasoning, bias in their response, to name a few.\n",
    "These pitfalls might occur due to:\n",
    "- **Domain knowledge deficit**\n",
    "- **Outdated information**\n",
    "- **Catastrophic forgetting**\n",
    "\n",
    "You can read more about these factors and solutions [here](https://arxiv.org/pdf/2312.05934.pdf). **Retrieval Augmented Generation (RAG)** is a proposed solution for that issue, helping practitioners use current domain-specific data to augment LLM capabilities. Many LLM applications require user-specific data that is not part of the model's training set as an external source, so that we can chat with our domain specific data (documents). In RAG, external data is retrieved and then passed to the LLM when doing the generation step.\n",
    "\n",
    "RAG should be seen as a pipeline or a system where applications typically have multiple stages: \n",
    "- **Before Query:** Loading and embedding relevant documents/information\n",
    "- **During Query:** Embed query, retrieve relevant information, and respond. \n",
    "\n",
    "A potential diagram is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8512151",
   "metadata": {},
   "source": [
    "<img src=\"./images/naiveRAG.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce92707",
   "metadata": {},
   "source": [
    "The elementary components of a RAG pipeline for Q&A task are: \n",
    "\n",
    "- **Indexing**\n",
    "    - Document loading & chunking\n",
    "    - Document embedding (we will explain this in Section 2.1.3. below) if a dense embedding model is used: Note that using a dense retriever is optional. You can use sparse embedding using algorithms like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) if you are looking for a retrieval method that doesn't need a neural network model for indexing. BM25 is a variant of [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). \n",
    "    - Embedding storing and indexing with a vector database (e.g. FAISS, Milvus, Chroma DB).\n",
    "- **Retrieval**\n",
    "    - Given a user query, relevant splits are retrieved from the vector database.\n",
    "- **Augmentation & Generation**\n",
    "  - Use the retrieved information to make a good prompt for a strong instruction-following LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c03de",
   "metadata": {},
   "source": [
    "Now let's go over each component of the RAG pipeline and go through the process of building it up for our use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270f57a",
   "metadata": {},
   "source": [
    "### 2.1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431e356",
   "metadata": {},
   "source": [
    "In this section we will go over how to load a document, chunk it and generate embeddings from the chunke documents (from our corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7ed96",
   "metadata": {},
   "source": [
    "#### 2.1.1. Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5be9e3",
   "metadata": {},
   "source": [
    "If we want to build a **Chat With Your Data** application and want LLMs to generate relevant and specific responses for our queries, we need the model to understand our domain and provide answers from our data instead of giving broad and generalized responses. \n",
    "\n",
    "In doing so, we start by loading our knowledge base of data that we want to ask queries against. Langchain provides [**document loaders**](https://python.langchain.com/docs/modules/data_connection/document_loaders/) to help process out data into `Document` values containing the appropriate text and metadata. There are a variety of such document loaders that can help us load anything from [**PDFs**](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) to [**web pages**](https://python.langchain.com/docs/use_cases/web_scraping) to even [**video transcriptions**](https://python.langchain.com/docs/integrations/document_loaders/youtube_transcript), but for this section we will stick to a simple textfile loader for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Provide the source of the txt document\n",
    "loader = TextLoader(\"/dli/task/dataset/gtc_sessions.txt\")\n",
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ea36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This is a Document object.\n",
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cb878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8031d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Uncomment the line below if you want to check the page content\n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f760837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cc69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(docs))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(docs[0].page_content)}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea20210",
   "metadata": {},
   "source": [
    "#### 2.1.2. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487ede6",
   "metadata": {},
   "source": [
    "Once documents have been loaded, they are often transformed into more usable formats. One method of transformation is known as **chunking**, which breaks down large pieces of text (i.e. a long document) into smaller segments called chunks. This technique is valuable because it helps optimize the relevance of the content returned from the vector database and limit the prompt length to something the LLM can reason with. \n",
    "\n",
    "Remember that LLMs can struggle to reason with long inputs that push beyond their rated context lengths, and even those that are rated for long context (i.e. GPT4) incur higher costs for the extra tokens and can exhibit instruction-following degredation. With chunking, you can take steps to alleviate this by retrieving and organizing only the most relevant information. There are different chunking strategies such as fixed size, sliding window, content-based chunking that work for different scenarios, and chunks size/preprocessing can be done to optimize performance for your specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f20494",
   "metadata": {},
   "source": [
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [**`RecursiveCharacterTextSplitter`** ](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us to split our document with preference for some natural stopping points that we want our chunks to follow (as much as possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9b4ed",
   "metadata": {},
   "source": [
    "Important parameters to know here are `chunkSize` and `chunkOverlap`. \n",
    "- `chunk_size` arg controls the max size (in terms of number of characters) of the final documents. \n",
    "- `chunk_overlap` specifies how much overlap there should be between chunks. \n",
    "\n",
    "This is often helpful to make sure that the text isn't split awkwardly. \n",
    "Langchain also allows you to [**split by tokens**](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token) using the [tiktoken](https://github.com/openai/tiktoken/tree/main)-backed `from_tiktoken_encoder()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b4866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=20, \n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\"],\n",
    ")\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbb880",
   "metadata": {},
   "source": [
    "Alternatively, you can also use `from_huggingface_tokenizer()` with a tokenizer of choice as shown here:\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-unsupervised')\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=500, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312899b-8b8b-4a58-8cc4-0daa6f6df83c",
   "metadata": {},
   "source": [
    "You might be asking why we set `chunk_size = 500`. We set the `chunk_size` less than 512 because the retriever model we use below supports a maximum input of 512 tokens. There is not a magic `chunk_size` or `chunk_overlap` value that we can set for every document/dataset. You can do some experiments with your own custom dataset and find out what values works for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73b4bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's check out how many chunks (doc splits) we have now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092edd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1884b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(docs_split)):\n",
    "    print(len(docs_split[i].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a78251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55633af0",
   "metadata": {},
   "source": [
    "By uncommenting the cell below you can check the number of tokens per chunked document split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1abe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "# for d in docs_split:\n",
    "#     print(\"The document is %s tokens\" % len(encoding.encode(d.page_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10770c",
   "metadata": {},
   "source": [
    "#### 2.1.3. Generating Query and Document Embeddings with a Dense Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5f6d5",
   "metadata": {},
   "source": [
    "Open-domain question answering (Q&A) relies on efficient passage/documents retrieval to select candidate contexts. [**`TF-IDF`** ](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [**`BM25`** ](https://en.wikipedia.org/wiki/Okapi_BM25) are traditional models that are known as lexical search methods and do not use dense embeddings, i.e. dense representations of the tokens or sentences. On the other hand, dense passage retrieval fulfills this gap and  it uses a dense encoder which maps any text passage to a `d` dimensional real-valued vectors. To learn more about `Dense Passage Retrieval for Open-Domain Question Answering` you can read this [paper](https://arxiv.org/pdf/2004.04906.pdf). An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are usually transformer encoders that process tokens of input text (for example, question, passage) to output an embedding. \n",
    "\n",
    "We can use `bi-encoder` architectures and generate embeddings both for query and text passages seperately. As the image shows, we feed query and chunked passages to different towers and then after embedding representation is generated for each query and each passage, we calculate the similarity (via cosine or dot-product) between query and passage. The score between the query and most relevant passages would be higher as opposed to the pairs that are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00144a21",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/biencoder.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc701e67",
   "metadata": {},
   "source": [
    "#### 2.1.3.1. Initialize the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c92636",
   "metadata": {},
   "source": [
    "[Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) for documents are created by vectorizing the document text; this vectorization captures the semantic meaning of the text. This allows you to quickly and efficiently find other pieces of text that are similar. Below, we use the\n",
    "`NVIDIA Retrieval QA Embedding Model`, which is part of **NVIDIA NeMo Retriever** and provides optimized, commercially-ready options for a production-ready information retrieval pipeline with enterprise support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd921a0b",
   "metadata": {},
   "source": [
    "The main requirement when initializing an embedding model is to provide the model name. An example is `nvolveqa_40k` below. \n",
    "The NVIDIA Retrieval QA Embedding Model is a transformer encoder - a finetuned version of [E5-Large-Unsupervised](https://huggingface.co/intfloat/e5-large-unsupervised), with 24 layers and an embedding size of 1024, which is trained on private and public datasets as described in the Dataset and Training section. It supports a maximum input of 512 tokens.\n",
    "\n",
    "\n",
    "For NVIDIA's embedding retriever model `nvolveqa_40k` model, you can also specify the model_type as `passage` or `query`. When doing retrieval, you will get best results if you embed the source documents with the passage type and the user queries with the query type.\n",
    "\n",
    "If not provided, the `embed_query` method will default to the query type, and the `embed_documents` method will default to the passage type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbdbb4-7012-4853-97f7-2e565f37aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NVIDIAEmbeddings.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4a708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\", model_type=\"passage\")\n",
    "query_embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\", model_type=\"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730d9d2",
   "metadata": {},
   "source": [
    "**Document Embedding**\n",
    "- **Purpose**: Tailored for longer-form or response-like content, including document chunks or paragraphs.\n",
    "- **Method**: Employs `embed_documents` for batch processing of documents.\n",
    "- **Role in Retrieval**: Acts as the \"value,\" representing the searchable content within the retrieval system.\n",
    "- **Usage Pattern**: Typically embedded en masse as a pre-processing step, creating a repository of document embeddings for future querying.\n",
    "\n",
    "**Query Embedding**\n",
    "- **Purpose**: Designed for embedding shorter-form or question-like material, such as a simple statement or a question.\n",
    "- **Method**: Utilizes `embed_query` for embedding each query individually.\n",
    "- **Role in Retrieval**: Functions as the \"key,\" facilitating the search or query process in a document retrieval framework.\n",
    "- **Usage Pattern**: Embedded dynamically, as needed, for comparison against a pre-processed collection of document embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62011f65",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's do a little exercise and generate embeddings for a set of queries and their corresponding documents/passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13512b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example queries and documents\n",
    "queries = [\n",
    "    \"What's the capital of Germany?\",\n",
    "    \"What kinds of food is Italy known for?\",\n",
    "    \"Who is the inventor of the World Wide Web?\",\n",
    "    \"What's the marathon distance?\",\n",
    "    \"What's the name of NVIDIA's famous AI Conference?\"\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"Berlin.\",\n",
    "    \"Italy is famous for pasta, pizza, gelato, and espresso.\",\n",
    "    \"Berners-Lee was honoured as the inventor of the WWW during the 2012 Summer Olympics opening ceremony\",\n",
    "    \"it is 42.195 kilometers.\",\n",
    "    \"It is the conference for the era of AI, or you can call it GTC for short.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c69f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embedding the queries\n",
    "q_embeddings = [query_embedder.embed_query(query) for query in queries]\n",
    "\n",
    "# Embedding the documents\n",
    "d_embeddings = document_embedder.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5bb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(d_embeddings), len(d_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c9fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min(d_embeddings[0]), max(d_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd935e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def plot_cross_similarity_matrix(emb1, emb2):\n",
    "    # Compute the similarity matrix between embeddings1 and embeddings2\n",
    "    cross_similarity_matrix = cosine_similarity(np.array(emb1), np.array(emb2))\n",
    "\n",
    "    # Plotting the cross-similarity matrix\n",
    "    plt.imshow(cross_similarity_matrix, cmap='plasma', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Cross-Similarity Matrix\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_cross_similarity_matrix(q_embeddings, d_embeddings)\n",
    "plt.xlabel(\"Query Embeddings\")\n",
    "plt.ylabel(\"Document Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35d2d0",
   "metadata": {},
   "source": [
    "### 2.1.4. Store Embeddings in the Vector Store [*(Retrieval)*](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7985b0",
   "metadata": {},
   "source": [
    "After this toy example showing how to generate document and query embeddings, let's move on with our chunked documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e95400",
   "metadata": {},
   "source": [
    "<img src=\"./images/rag_vectordb.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49eaa9",
   "metadata": {},
   "source": [
    "As the diagram explains, once we chunk our documents, the next step is to generate embeddings for each chunk using our document embedding model and then index these embeddings using a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b9428",
   "metadata": {},
   "source": [
    "When a user sends in their query, the query is embedded using the query embedding model. Note that here in the bi-encoder, we use the same embedding model for both the passage and query embedding routines. Once we create the embeddings both for queries and documents, then we can find semantically similar (relevant) documents to the user's query by applying a similarity metric.\n",
    "\n",
    "Once the document embeddings are generated, they are stored in a vector store so that at query time we can:\n",
    "1) Embed the user query <br>\n",
    "2) Retrieve the embedding vectors that are most similar to the embedding query using a similarity score\n",
    "\n",
    "A vector store takes care of storing the embedded data and performing a vector search. LangChain provides support for a [selection of vector stores](https://python.langchain.com/docs/integrations/vectorstores/) among which we will choose [FAISS](https://github.com/facebookresearch/faiss) for its simplicity. [Milvus](https://milvus.io/docs/integrate_with_langchain.md) is another alternative with strong scaling features and [NVIDIA RAPIDS RAFT acceleration](https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/) for larger-scale deployments, but FAISS is a good starting point for a single-user application, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b890c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8024047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import (\n",
    "    DistanceStrategy,\n",
    "    maximal_marginal_relevance,\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(\n",
    "    docs_split, \n",
    "    embedder, \n",
    "    distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92dc16",
   "metadata": {},
   "source": [
    "Check out whether the index was trained on our data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229598a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010e903",
   "metadata": {},
   "source": [
    "Check out the embedding dimension. `1024` comes from the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8c981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.index.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debb9c3",
   "metadata": {},
   "source": [
    "Print out  the total number of vectors added to the index. Since we have 12 chunked doc splits, we get 12 indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf17aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5d6a5",
   "metadata": {},
   "source": [
    "There are some FAISS specific methods like `similarity_search_with_score` which returns not only the documents but also the distance or similarity score of the query to them. Let's return two most similar document chunks by setting `k=2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f930cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What presentations are given about guardrails?\"\n",
    "docs_and_scores = db.similarity_search_with_score(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3342b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66954962",
   "metadata": {},
   "source": [
    "Since we use `MAX_INNER_PRODUCT` as distance strategy, we expect the higher score the better because a similarity score returned not a distance score. Note that if the embeddings are normalized the `max_inner_product` corresponds to `cosine` similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b0b65",
   "metadata": {},
   "source": [
    "### 2.2. Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a54af8",
   "metadata": {},
   "source": [
    "Next, the query embedding is used to search a vector database that retrieves a small number of the most relevant document chunks to the user’s query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b41c5",
   "metadata": {},
   "source": [
    "<img src=\"./images/embedder.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0eae1",
   "metadata": {},
   "source": [
    "Let's first convert the vectorstore into a `Retriever` which will return relevant documents for a provided query input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a02a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "relevant_docs = retriever.invoke(\"What presentations are given about guardrails?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07b939",
   "metadata": {},
   "source": [
    "Uncomment the cell below to see what kinds of docs were retrieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efb148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# relevant_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb1457",
   "metadata": {},
   "source": [
    "### 2.3. Use an LLM to generate a response for a user query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefe0a2",
   "metadata": {},
   "source": [
    "<img src=\"./images/rag_llm.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff42246",
   "metadata": {},
   "source": [
    "The last step of the RAG pipeline is to generate responses back to the users. At this point, we create an expanded prompt (with the retrieved top-k chunks from retrieval step) for the LLM to generate a relevant and accurate response. A few things to consider:\n",
    "\n",
    "- The proper and explicit human-written instruction format might affect the final response quality. It is also important to follow the recommended chat templates by the model's developers.\n",
    "- Sending a large prompt to the LLM might hurt the final response accuracy due to `lost in the middle phenomenon` (see the [paper](https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf) for more information). The paper states that the large language models are often better at retrieving and using information at the start or end of\n",
    "the input contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025bd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Answer solely based on the following context.: {context}\",\n",
    "    ),\n",
    "    (\"user\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def print_return(d):\n",
    "    print(repr(d))\n",
    "    return d\n",
    "\n",
    "\n",
    "model = ChatNVIDIA(model=\"ai-mistral-7b-instruct-v2\", max_tokens=1024)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke(\"what's the projected value of the market of Gen AI and LLMs in healthcare?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5d093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke(\"Who is the VP of NVIDIA Healthcare and Life Sciences?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c4996",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**3. Evaluation of the RAG Pipeline**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a4159",
   "metadata": {},
   "source": [
    "With LangChain and the out-of-the-box NVIDIA endpoints, it should be pretty easy (and maybe even *fun*) to build a proof-of-concept RAG pipeline. However, it's also important to evaluate our pipeline to make sure it's ready to go into production. Noting that the RAG pipeline consists of the `Retrieval` and `Generator` components, we need to consider what all a proper evaluation might entail?\n",
    "\n",
    "Given a user question, the retriever finds relevant passages from a corpus (i.e., the knowledge base that we chunk and feed to the embedding model) and the language model uses these retrieved passages to generate a response. \n",
    "\n",
    "**Why Do We Want To Evaluate RAG?**\n",
    "\n",
    "- Make sure the retrieved chunks and generated responses are of high quality.\n",
    "- Know where the bottlenecks are and how we can improve it (retrieval? generator? both?)\n",
    "- Compare different models to pick the best options for our use cases. \n",
    "\n",
    "What else do you think we can add into this list?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479cb20",
   "metadata": {},
   "source": [
    "#### **What Can Be Evaluated?**\n",
    "\n",
    "Let's consider what can be evaluated given the retrieved context, generated answer, the user query, and ground truth. The figure below gives a high level RAG Triad as presented by [TruLens](https://www.trulens.org/trulens_eval/core_concepts_rag_triad/). We added more metrics into the figure that one can consider of calculating. \n",
    "\n",
    "<img src=\"./images/rag_triad.png\" width=500>\n",
    "\n",
    "\n",
    "Note that this Triad does not show metrics that can be calculated using `ground_truth`. We can also calculate some metrics. e.g. `answer_similarity` using the `ground_truth` and `generated answer`.\n",
    "\n",
    "Let's do some exercises to evaluate the **Retrieval** and **Generator** components of our RAG pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d16d85",
   "metadata": {},
   "source": [
    "### 3.1. Evaluating the Retrieval component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f6e8d",
   "metadata": {},
   "source": [
    "Since all retrievers in RAGs have a critical need to solve for the semantic understanding of the raw text, it is paramount to have a systematic evaluation process to choose the right `retriever` model. As we have just experienced, retrieval is the part where we return the relevant chunks for a given query and then we feed those chunks to the generator.\n",
    "\n",
    "The best data to evaluate retrieval is your own. Ideally, you would want to build a clean and labeled evaluation dataset that best reflects what you would see in production. It is best to build a custom benchmark to evaluate the quality of different retrievers, which can vary greatly depending on the domain and task.\n",
    "\n",
    "Without well-labeled evaluation data, many turn to popular benchmarks as proxies for evaluating retrievers. [MTEB](https://github.com/embeddings-benchmark/mteb) is a benchmark that spans 8 embedding tasks covering a total of 56 datasets and 112 languages. However, a crucial question is: `Do the datasets in these benchmarks truly represent our workload?`, because evaluating performance on irrelevant cases can lead to false confidence in your RAG pipelines.  \n",
    "\n",
    "The type of data you encounter in production plays a crucial role in determining the relevance of academic benchmarks. We always talk about the quality/performance of the model but we underestimate the quality of the data. As we all aware of the `garbage in / garbage out` concept in our ML practices, that applies to RAG applications as well.\n",
    "\n",
    "Additionally, the choice of metrics should be use-case dependent as well when evaluating your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e7e4d",
   "metadata": {},
   "source": [
    "<img src=\"./images/metrics.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d5df",
   "metadata": {},
   "source": [
    "We can summarize some best practices when evaluating the Retrieval component as:\n",
    "1. **Create your evaluation dataset:** The quality of dataset is important. The evaluation/test dataset should reflect the domain that you want to use RAG for. \n",
    "One should consider creating a dataset of realistic questions while curating the amount of ambiguity, divergence, and difficulty for your typical and edge use-cases. <br> Generating `human-annotated` data would be preferred but it also slow and resource-intensive, wereas synthetic data generation using LLMs has been used and is being researched with some promising results. \n",
    "2. **Define the accuracy metrics**: To assess the quality of our system, we need to define some metrics to either quantitatively or qualitatively measure the performance. Well known quantitative metrics can be classified as `rank-aware` and `rank-agnostic`. Note that `recall` and `precision` do not take the order of the returned documents into consideration, whereas `NDCG` and `MAP` are order-aware metrics that are suitable for document ranking.\n",
    "3. **Define the evaluation tool:** In addition to writing your own custom code to evaluate the retrieval and generator, open-source libraries like [Ragas](https://github.com/explodinggradients/ragas), [TruLens](https://github.com/truera/trulens), [Phoenix Evals](https://docs.arize.com/phoenix/use-cases/rag-evaluation) can also be used to automate the process and provide useful starting points. Note that these libraries rely on a strong and consistent LLM which they use in an LLM-as-a-Judge style to evaluate your pipeline.\n",
    "4. **Human-in-the-loop evaluation:** Although it is expensive and time consuming, it might still be very useful to incorporate human judgement in the evaluation process. This might be required in some edge cases where the generated results can create critical consequences, such as in the medical domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0a1d7",
   "metadata": {},
   "source": [
    "Although various metrics might be suitable for your specific use cases, we will focus on two relatively simple but popular metrics that you're likely to encounter in information retrieval: **`Recall`** and **`NDCG`**.\n",
    "\n",
    "A rank-agnostic metric, `Recall`, measures the percentage of the relevant results retrieved:\n",
    "\n",
    "$$Recall = \\frac{Number\\ of\\ Relevant\\ Items\\ Retrieved}{Total\\ Number\\ of\\ Relevant\\ Items}$$\n",
    "\n",
    "`NDCG's` advantage is that it can handle both binary and graded relevance. If you want to learn more about the NDCG mathematical formulas behind the metric, this [page](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) is a helpful resource. Furthermore this [blog post](https://www.pinecone.io/learn/offline-evaluation/) also gives useful explanations with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7186f4",
   "metadata": {},
   "source": [
    "### 3.2. Retrieval Evaluation with NVIDIA Embedding Microservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf8288",
   "metadata": {},
   "source": [
    "In this section, we will perform retrieval evaluation using one of the [BEIR benchmark]((https://github.com/beir-cellar/beir) datasets, [FiQA-2018](https://sites.google.com/view/fiqa/) (Financial Opinion Mining and Q&A). [BEIR](https://github.com/beir-cellar/beir) benchmark has 18 publicly available datasets for diverse information retrieval tasks and domains. Each dataset caters toward measuring the performance of various applications of an embedding model— retrieval, clustering, summarization. Given the focus on RAG, you should consider which performance metrics and which datasets are most useful for evaluating a Question-Answering (QA) retrieval solution aligned with your use case. In the retrieval task, `NDCG@10` is the metric used across several different models. You can check out the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) of the `Retrieval task`, which is our focus here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f9cc4",
   "metadata": {},
   "source": [
    "Beir Github [repo](https://github.com/beir-cellar/beir) provides evaluation code for the beir benchmark datasets. These datasets are in a special format, we can call it as `beir format` for now. We actually took the code [here](https://github.com/beir-cellar/beir/blob/main/examples/retrieval/evaluation/custom/evaluate_custom_model.py) to create a custom class to run evaluation on a model with the query and passage prefixes. Note that Beir evaluation does not apply chunking since it evaluates the retrieval with already chunked corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fdc27",
   "metadata": {},
   "source": [
    "### 3.2.1. NVIDIA Embedding Microservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a965f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "NVIDIA NeMo™ microservices is a collection of containerized software to easily and rapidly build and deploy large language model (LLM) workloads for enterprise use cases. As a semantic-retrieval microservice, [NeMo Retriever](https://developer.nvidia.com/nemo-microservices-early-access) helps generative AI applications provide more accurate responses through NVIDIA-optimized algorithms. Developers using the microservices can connect their AI applications to business data wherever it resides across clouds and data centers. It adds NVIDIA-optimized RAG capabilities to AI foundries and is part of the NVIDIA AI Enterprise software platform, available in AWS Marketplace.\n",
    "\n",
    "The `NVIDIA NeMo Retriever Embedding Microservice` brings the power of state-of-the-art text embedding to your applications, providing unmatched natural language processing and understanding capabilities. Whether you’re developing semantic search, Retrieval Augmented Generation (RAG) pipelines—or any application that needs to use text embeddings—NeMo Retriever Embedding has you covered. Built on the NVIDIA software platform incorporating CUDA, TensorRT, and Triton, NeMo Retriever Embedding brings state of the art GPU accelerated Text Embedding model serving.\n",
    "\n",
    "**NeMo Retriever Embedding uses NVIDIA’s [TensorRT](https://developer.nvidia.com/tensorrt) built on top of the [Triton Inference Server](https://developer.nvidia.com/triton-inference-server) for optimized inference of text embedding models**.\n",
    "\n",
    "- **Scalable Deployment:** Whether you’re catering to a few users or millions, NeMo Retriever Embedding can be scaled seamlessly to meet your demands.\n",
    "\n",
    "- **Flexible Integration:** Easily incorporate NeMo Retriever Embedding into existing workflows and applications, thanks to the OpenAI-compliant API endpoints.\n",
    "\n",
    "- **Secure Processing:** Your data’s privacy is paramount. NeMo Retriever Embedding ensures that all inferences are processed securely, with rigorous data protection measures in place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5fdff",
   "metadata": {},
   "source": [
    "Let's check if the embedding MS is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -v http://embedding-ms:12345/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa788d2-67d2-4bf7-9140-472d535e0f86",
   "metadata": {},
   "source": [
    "Let's send a toy text snippet to embedding microservice as a request, and return an embedding vector for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc3e2e-091c-4fdb-bf02-788a53b17a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import Dict, List, Literal, Optional\n",
    " \n",
    "import numpy as np\n",
    "import requests\n",
    " \n",
    "def _embed(\n",
    "    text: List[str],\n",
    "    prefix = 'passage',\n",
    "    model_name = '',\n",
    " \n",
    ") -> List[np.ndarray]:\n",
    "    headers: Dict[str, str] = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model_name, \"input\": text, \"encoding_format\": \"base64\"}\n",
    "    payload[\"input_type\"] = prefix\n",
    "    payload[\"truncate\"] = \"END\"\n",
    " \n",
    "    response = requests.request(\"POST\", \"http://embedding-ms:12345/v1/embeddings/\", headers=headers, json=payload, timeout=600)\n",
    " \n",
    "    if response.status_code != requests.status_codes.codes.ok:\n",
    "        msg = f\"Error calling NeMo Retriever Embedding Microservice: \\n{response.text}\"\n",
    "        raise Exception(msg)\n",
    " \n",
    "    response_data = response.json()[\"data\"]\n",
    " \n",
    "    embeddings = [\n",
    "        np.frombuffer(base64.b64decode(emb[\"embedding\"]), dtype=np.float32)\n",
    "        for emb in sorted(response_data, key=lambda o: o[\"index\"])\n",
    "    ]\n",
    " \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51b001-df48-42d5-838c-0d37b43677b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_toy = _embed([\"I am attending NVIDIA GTC this year!\"], model_name='nvolveqa')\n",
    "print(embeddings_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e3ed3-7110-4860-968d-ffbfcc117f16",
   "metadata": {},
   "source": [
    "Let's check out the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ebe77-b89f-4b45-8f6e-9f865d9b33ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_toy[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e348a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Generate Embeddings for Beir dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad9121",
   "metadata": {},
   "source": [
    "The following command calculates the embeddings using our `nvolve40k` model for `FiQA` dataset corpus and queries, and then it saves them as numpy arrays on the disk. The script below first downloads and unzips one of the smaller Beir benchmark datasets, [FiQA-2018](https://sites.google.com/view/fiqa/)  dataset, and then it runs evaluation on it using the `nvolve40k` retriever model. In order to generate embeddings, it sends requests to the running `NVIDIA Embedding Microservice` and gets the responses back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6443af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./beir/extract_embeddings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17679f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Perform Evaluation** \n",
    "\n",
    "Once we generated the embeddings, now we can perform the evaluation using our custom evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e6ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./beir/run_beir_benchmark.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da165907",
   "metadata": {},
   "source": [
    "Since `beir` is using `pytrec_eval` to evaluate the models, you can visit [pytrec_eval library](https://github.com/cvangysel/pytrec_eval) to understand how Beir evaluation metrics are calculated in a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abab1a4",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "In the section above, we evaluated the Retrieval component of the RAG pipeline using custom Beir evaluation script and generated `recall@k` and `Ndcg@k` metrics. \n",
    "If your retriever model results in higher Recall@k but lower NDCG@k that means it is selecting the right documents but not returning them in the desired order. `NDCG@k` is an important metric since we want the most relevant documents to be ranked in higher positions. To boost your NDCG metric, you can add a `reranker` model in your pipeline to rerank the fetched relevant documents. You can explore the Nvidia Reranking Microservice for that purpose.\n",
    "\n",
    "However if the recall metric is low, that means retriever is not doing a good job finding the relevant documents. In that case, you might want to \n",
    "- Choose a stronger retriever model\n",
    "- Perform a hybrid retrieval approach (for example combination of lexical + dense retriever models)\n",
    "- Finetune your retriever model with your custom data or with a publicly available datasets that can be good represent of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185ae03",
   "metadata": {},
   "source": [
    "### 3.3. RAG Evaluation with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dd63e",
   "metadata": {},
   "source": [
    "We learned how we can calculate offline metrics for the document retrieval step, but what about evaluating the quality (accuracy, relevancy, faithfullness, etc.) of the final response generated by the Generator? Traditional metrics, `F1 score`, `Exact match (EM)`, may not fully capture the quality of responses generated from the Generator component, i.e. LLM.\n",
    "\n",
    "`LLMs evaluate LLMs` paradigm started to be a practice that leverages a powerful LLM to generate proxy targets based on some context. In the case of our QA bot, we can ask an LLM to generate question-answer pairs. One thing we need to pay attention is prompting the LLM models correctly when using them. LLM models can be sensitive the prompt template.\n",
    "\n",
    "There are some open source libraries such as [Ragas](https://github.com/explodinggradients/ragas), [TruLens](https://github.com/truera/trulens), [Phoenix Evals](https://docs.arize.com/phoenix/use-cases/rag-evaluation) to automate the evaluation process of RAG systems. \n",
    "\n",
    "[Ragas](https://github.com/explodinggradients/ragas) is a framework that is built to help users to evaluate their RAG pipelines. In this tutorial we also demonstrate how we can generate metrics using Ragas and Langchain. Ragas provides different metrics for retrieval and generator. It calculates diffent metrics for Generator evaluation such as `AnswerSimilarity` and `Faithfullness`. Let's calculate these two metrics for our sample dataset, below and try to understand how it works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0e23d",
   "metadata": {},
   "source": [
    "Ragas provides metrics for both Retrieval and Generator component. The following metrics can be used for evaluating Generator component: <br>\n",
    "\n",
    "- [Answer Similarity](https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html): Scores the semantic similarity of ground truth with the generated answer. <br>\n",
    "- [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html): Measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. The higher the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcded6",
   "metadata": {},
   "source": [
    "**Preparing our dataset for Ragas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d14c5d",
   "metadata": {},
   "source": [
    "Ragas requires datasets to be in a special format to evaluate them. Data preparation guide can be found [here](https://docs.ragas.io/en/stable/howtos/applications/data_preparation.html). In this exercise, we are using a sample `ragas_eval` dataset called [explodinggradients/fiqa](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval) which is available on HuggingFace. We have already pulled in the parquet version of the dataset, so let's take a quick look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837a530-9829-44d3-97ad-7916d231bceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e212b-3ff4-417a-bb35-59a32cd9187d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the type of the dataset is DatasetDict\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff692924-6b89-4efb-a4f5-c8869f30db4d",
   "metadata": {},
   "source": [
    "For the next part, we'll need to convert it to a pandas dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30410da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset= dataset['baseline'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582eaf8-ad3a-40be-891a-f619a9d1d057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ba3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275b942-1de0-435d-945a-394a461303b1",
   "metadata": {},
   "source": [
    "This sample dataset has 30 rows which means it consists of 30 questions and corresponding ground_truths, answer and contexts columns. This is a sample dataset from FIQA2018 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712339f",
   "metadata": {},
   "source": [
    "The recent versions of ragas wants the `ground_truths` column name to be `ground_truth`. Let's create the `ground_truth` column and drop the `ground_truths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161303e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"ground_truth\"] = dataset[\"ground_truths\"].map(lambda x: x[0])\n",
    "dataset.drop(columns=['ground_truths'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db7f0b",
   "metadata": {},
   "source": [
    "<font color='blue'>**Note that** </font> here we are going to use the `answer` column coming from this dataset but in real-life we should generate our own responses (answers) from our RAG pipeline and then calculate the metrics for generator component using any evaluation tool. This can be a good exercise for you for later. You can actually try to generate answers for each question in this dataset and then rerun the steps below again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debde31",
   "metadata": {
    "tags": []
   },
   "source": [
    "Convert the dataset to a `Dataset` object to perform evaluation with `Ragas.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586c208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "eval_dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877734c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052e0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef38106",
   "metadata": {},
   "source": [
    "**Calculate the answer similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffc265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Useful RAGAS Imports\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    AnswerCorrectness,\n",
    "    AnswerSimilarity\n",
    ")\n",
    "\n",
    "# Support Utilities to connect to LangChain\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07f847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Replace the default LLM (gpt-3.5-turbo-16k) with another model\n",
    "llm = LangchainLLMWrapper(ChatNVIDIA(model=\"ai-mistral-7b-instruct-v2\", max_tokens=1024))\n",
    "\n",
    "## We can use our own embedding model for similarity calculations.\n",
    "embedder = LangchainEmbeddingsWrapper(NVIDIAEmbeddings(model=\"ai-embed-qa-4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1c6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_similarity = AnswerSimilarity(llm=llm, embeddings=embedder)\n",
    "ans_sim = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[answer_similarity],\n",
    "    llm = llm,\n",
    "    embeddings=embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab03d2",
   "metadata": {},
   "source": [
    "Print the average `answer_similarity` metric over entire queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfaccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ans_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f54690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_answer_sim = ans_sim.to_pandas()\n",
    "df_answer_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852bc9b",
   "metadata": {},
   "source": [
    "**Calculate the faithfullness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf9f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faith_mtr = Faithfulness(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b801bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faith_mtr.long_form_answer_prompt.instruction = (\"Create one or more statements from each sentence in the given answer.\"\n",
    "    \" Response MUST begin with square [ + curly bracket and end in curly + square ] bracket!\"\n",
    ")\n",
    "faith_mtr.nli_statements_message.instruction = (\n",
    "    \"Natural language inference. Use only 'Yes' (1) and 'No' (0) as verdict.\"\n",
    "    \" Response must begin with square bracket!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d00399",
   "metadata": {},
   "source": [
    "We can print out the few-shot examples that are already given in the Ragas source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716ff51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faith_mtr.long_form_answer_prompt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5a4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#faith_mtr.nli_statements_message.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c6f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = evaluate(\n",
    "    eval_dataset, \n",
    "    metrics=[faith_mtr],\n",
    "    llm = llm,\n",
    "    embeddings=embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bf53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2875b",
   "metadata": {},
   "source": [
    "For better readability we can convert the results to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd67422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_faith = results.to_pandas()\n",
    "df_faith.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ecd49",
   "metadata": {},
   "source": [
    "One further exercise you can do is to check out the samples with the low scores. You can examine if there are any noisy samples, which may indicate a need for further data curation. You can also deep dive in [the source code](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py) to understand how an LLM model can be used as a judge to calcuate a certain accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707433d9",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**Summary**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7049f",
   "metadata": {},
   "source": [
    "That's it! You have done a great job by finishing this tutorial! \n",
    "\n",
    "Though we covered quite a few pieces of the RAG pipeline, there are still more components we could have incorporated. Some interesting further efforts might include:\n",
    "- `Hybrid Search (lexical + dense retrievers)`\n",
    "- `Guardrails`\n",
    "- `Post-retrieval optimization (e.g. reranking)` to create an Advanced Rag pipeline. \n",
    "\n",
    "[NVIDIA NeMo framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) provides these functionalities to build, customize, and deploy generative AI models anywhere via its  microservices and open source libraries. Check out the resources below to learn more, and enjoy the rest of the conference! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943ecfa",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**Resources**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2873cf",
   "metadata": {},
   "source": [
    "[1] NVIDIA Retrieval QA Embedding on [NGC AI Playground](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k).<br>\n",
    "[2] Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model, NVIDIA Technical [blog post](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/).<br>\n",
    "[3] Evaluating Retriever for Enterprise-Grade RAG, NVIDIA Technical [blog post](https://developer.nvidia.com/blog/evaluating-retriever-for-enterprise-grade-rag/). <br>\n",
    "[4] Prompt Engineering with LLaMA-2, DLI [course](https://courses.nvidia.com/courses/course-v1:DLI+S-FX-12+V1/).<br>\n",
    "[5] Rapid Application Development with Large Language Models (LLMs), DLI [course](https://courses.nvidia.com/courses/course-v1:DLI+C-FX-09+V1/).<br>\n",
    "[6] Model Parallelism: Building and Deploying Large Neural Networks, DLI [course](https://www.nvidia.com/en-us/training/instructor-led-workshops/model-parallelism-build-deploy-large-neural-networks/).<br>\n",
    "[7] [NVIDIA NeMo Microservices](https://developer.nvidia.com/nemo-microservices-early-access), which offers microservice spinup routines that can be deployed on local compute and function similar to AI Playground.<br> \n",
    "[8] [NVIDIA AI Workbench](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/)allows for quick and simple model customization workflows that can greatly improve RAG model components for your specific use-cases.<br>\n",
    "[9] [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.<br>\n",
    "[10] NVIDIA Generative AI Examples [Repo](https://github.com/NVIDIA/GenerativeAIExamples).<br>\n",
    "[11] [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/), The “Operating System” for Enterprise AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
